# SetPINNs ä»£ç å¤ç°æ£€æŸ¥æŠ¥å‘Š

## ä¸€ã€æ¨¡å‹æ¶æ„å‚æ•°å¯¹æ¯”

### è®ºæ–‡ Table \ref{tab:model-hyperparameters}
- **embedding size**: 32
- **hidden size**: 512  
- **\# of encoder**: 1
- **head**: 2
- **set size**: 4

### ä»£ç å®ç° (`setpinn/models/__init__.py:37`)
```python
SetPinns(d_out=1, d_hidden=32, d_model=128, N=1, heads=2)
```

**âŒ ä¸ä¸€è‡´é—®é¢˜ï¼š**
- è®ºæ–‡ï¼šembedding size = 32, hidden size = 512
- ä»£ç ï¼šd_model (embedding) = 128, d_hidden = 32
- **ä¸¥é‡ä¸åŒ¹é…ï¼** ä»£ç çš„embeddingç»´åº¦æ˜¯è®ºæ–‡çš„4å€ï¼Œhiddenç»´åº¦æ˜¯è®ºæ–‡çš„1/16

**å¯èƒ½çš„è§£é‡Šï¼š**
- å‚æ•°å‘½åä¸åŒï¼šè®ºæ–‡çš„"hidden size"å¯èƒ½æŒ‡FFNçš„ä¸­é—´å±‚ï¼ˆä»£ç ä¸­d_ff = 2*d_model = 256ï¼‰
- ä½†embedding sizeçš„å·®å¼‚ä»ç„¶å¾ˆå¤§ï¼ˆ32 vs 128ï¼‰

---

## äºŒã€è®­ç»ƒè¶…å‚æ•°å¯¹æ¯”

### è®ºæ–‡ Table \ref{tab:training-hyperparameters}
- **Adam Iterations**: 100
- **L-BFGS Iterations**: 2000
- **Train:Test Split (grid)**: $50\times50$:$101\times101$
- **Train:Test Split (number of samples)**: 2500:10201
- **$\lambda_{\Omega}$**: 1
- **$\lambda_{\Omega_0}$**: 1
- **$\lambda_{\partial\Omega}$**: 1

### ä»£ç é»˜è®¤å€¼ (`train.py`)
- **`--adam_steps`**: 50000 (é»˜è®¤)
- **`--training_iterations`**: 500 (é»˜è®¤ï¼ŒL-BFGS)
- **`--res_points`**: 50 (é»˜è®¤) â†’ 50Ã—50 = 2500 âœ“
- **`--test_points`**: 100 (é»˜è®¤) â†’ 100Ã—100 = 10000 âœ— (åº”è¯¥æ˜¯101)
- **Lossæƒé‡**: `sum(losses)` â†’ ç­‰ä»·äº Î»=1 âœ“

**âŒ ä¸ä¸€è‡´é—®é¢˜ï¼š**
1. **Adamè¿­ä»£æ¬¡æ•°**: è®ºæ–‡100ï¼Œä»£ç é»˜è®¤50000ï¼ˆå·®500å€ï¼ï¼‰
2. **L-BFGSè¿­ä»£æ¬¡æ•°**: è®ºæ–‡2000ï¼Œä»£ç é»˜è®¤500ï¼ˆå·®4å€ï¼‰
3. **æµ‹è¯•ç‚¹æ•°**: è®ºæ–‡101Ã—101=10201ï¼Œä»£ç é»˜è®¤100Ã—100=10000

---

## ä¸‰ã€æŸå¤±å‡½æ•°å®ç°å¯¹æ¯”

### è®ºæ–‡ Equation (7) - Localized Residual Energy
$$\mathcal{E}_X(E_k, u_\theta) := \int_{E_k} \bigl\|\mathcal{O}_X(u_\theta)(x)\bigr\|^2 dx \approx \frac{|E_k|}{m_k} \sum_{i=1}^{m_k} \bigl\|\mathcal{O}_X(u_\theta)(x_k^{(i)})\bigr\|^2$$

### è®ºæ–‡ Equation (8) - Total Loss
$$\mathcal{L}_{\text{SetPINNs}} = \sum_{X \in \{\Omega,\, \Omega_0,\, \partial\Omega\}} \frac{\lambda_X}{K} \sum_{k=1}^{K} \mathcal{E}_X(E_k, u_\theta)$$

### ä»£ç å®ç° (`setpinn/losses.py` å’Œ `train.py:167`)
```python
# æŸå¤±å‡½æ•°ä¸­
loss_res = torch.mean((u_tt - 4 * u_xx) ** 2)  # ç›´æ¥å¯¹æ‰€æœ‰ç‚¹æ±‚mean

# è®­ç»ƒå¾ªç¯ä¸­
total_loss = sum(losses)  # ç›´æ¥ç›¸åŠ ï¼Œç›¸å½“äºÎ»=1
```

**âš ï¸ æ½œåœ¨é—®é¢˜ï¼š**
- è®ºæ–‡å¼ºè°ƒ**æŒ‰å…ƒç´ ï¼ˆsetï¼‰åˆ†åˆ«è®¡ç®—æŸå¤±å†å¹³å‡**
- ä»£ç ç›´æ¥å¯¹æ‰€æœ‰collocationç‚¹æ±‚mean
- **æ•°å­¦ä¸Šå¯èƒ½ç­‰ä»·**ï¼ˆå¦‚æœæ¯ä¸ªsetçš„ç‚¹æ•°ç›¸åŒä¸”å…ƒç´ å¤§å°ç›¸åŒï¼‰ï¼Œä½†**å®ç°æ–¹å¼ä¸ç¬¦åˆè®ºæ–‡æè¿°**
- è®ºæ–‡æ˜ç¡®æåˆ°"aggregates residuals within each element"ï¼Œä»£ç æ²¡æœ‰ä½“ç°è¿™ä¸€ç‚¹

**åº”è¯¥çš„å®ç°æ–¹å¼ï¼š**
```python
# ä¼ªä»£ç ç¤ºæ„
for each set k:
    set_loss_k = (|E_k|/m_k) * sum(residual^2 for points in set k)
total_loss = mean(set_loss_k for all sets)
```

---

## å››ã€Element-Aware Sampling (EAS) å®ç°

### è®ºæ–‡ Definition 2
- åœ¨æ¯ä¸ªå…ƒç´ $E_k$å†…å‡åŒ€é‡‡æ ·$m_k$ä¸ªç‚¹
- è¦æ±‚é‡‡æ ·å¯†åº¦å‡åŒ€ï¼š$\frac{m_k}{|E_k|} = \frac{m_j}{|E_j|}$ for all k,j

### ä»£ç å®ç° (`setpinn/data.py:44-58`)
```python
def eas_sampling(x, t, set_size=4):
    # éå†ç½‘æ ¼ä¸­çš„æ¯ä¸ªæ­£æ–¹å½¢
    for i in range(N - 1):
        for j in range(M - 1):
            # åœ¨æ¯ä¸ªæ­£æ–¹å½¢å†…å‡åŒ€é‡‡æ ·set_sizeä¸ªç‚¹
            xs = np.random.uniform(x0, x1, set_size)
            ts = np.random.uniform(t0, t1, set_size)
```

**âœ“ åŸºæœ¬ç¬¦åˆï¼š**
- å®ç°äº†åœ¨æ¯ä¸ªå…ƒç´ å†…å‡åŒ€é‡‡æ ·
- ä½¿ç”¨`set_size=4`ä¸è®ºæ–‡ä¸€è‡´
- ä½†è®ºæ–‡æåˆ°"proportional allocation": $m_k = M \frac{|E_k|}{|\Omega|}$ï¼Œä»£ç ä¸­æ‰€æœ‰å…ƒç´ å¤§å°ç›¸åŒï¼ˆ$2\times2$åˆ†å‰²ï¼‰ï¼Œæ‰€ä»¥è‡ªç„¶æ»¡è¶³

---

## äº”ã€æ•°æ®é›†åˆ†å‰²

### è®ºæ–‡
- **Train grid**: $50\times50$ = 2500 points
- **Test grid**: $101\times101$ = 10201 points

### ä»£ç 
- **Train**: `res_points=50` â†’ 50Ã—50 = 2500 âœ“
- **Test**: `test_points=100` â†’ 100Ã—100 = 10000 âœ—

---

## å…­ã€å…¶ä»–è®¾ç½®

### Set Size
- è®ºæ–‡ï¼š4 points per element âœ“
- ä»£ç ï¼š`set_size=4` âœ“

### Feed Forward Network
- è®ºæ–‡ï¼šæœªæ˜ç¡®è¯´æ˜FFNç»“æ„
- ä»£ç ï¼š`d_ff = 2 * d_model` (æ ‡å‡†Transformerè®¾è®¡) âœ“

### Activation Function
- è®ºæ–‡ï¼šæœªæ˜ç¡®è¯´æ˜
- ä»£ç ï¼š`WaveAct` (sin/cosç»„åˆ) âœ“

### Optimizer Settings
- **L-BFGS**: `line_search_fn="strong_wolfe"` âœ“ (ä¸è®ºæ–‡ä¸€è‡´)
- **Adam**: ä½¿ç”¨warmupå’Œcosine schedulerï¼ˆè®ºæ–‡æœªæ˜ç¡®è¯´æ˜ï¼Œä½†å¸¸è§åšæ³•ï¼‰

---

## ä¸ƒã€æ€»ç»“

### âœ… ç¬¦åˆè®ºæ–‡çš„éƒ¨åˆ†ï¼š
1. Set size = 4
2. Encoderå±‚æ•° = 1
3. Attention heads = 2
4. L-BFGSä½¿ç”¨strong Wolfeçº¿æœç´¢
5. Lossæƒé‡Î»=1
6. Element-aware samplingç­–ç•¥æ­£ç¡®
7. è®­ç»ƒæ•°æ®ç‚¹æ•° 50Ã—50 = 2500

### âŒ ä¸ç¬¦åˆè®ºæ–‡çš„éƒ¨åˆ†ï¼š
1. **æ¨¡å‹æ¶æ„å‚æ•°ä¸¥é‡ä¸åŒ¹é…**ï¼š
   - Embedding size: 32 (è®ºæ–‡) vs 128 (ä»£ç )
   - Hidden size: 512 (è®ºæ–‡) vs 32 (ä»£ç )
   
2. **è®­ç»ƒè¿­ä»£æ¬¡æ•°å·®å¼‚å·¨å¤§**ï¼š
   - Adam: 100 (è®ºæ–‡) vs 50000 (ä»£ç é»˜è®¤)
   - L-BFGS: 2000 (è®ºæ–‡) vs 500 (ä»£ç é»˜è®¤)
   
3. **æµ‹è¯•ç‚¹æ•°ä¸åŒ¹é…**ï¼š
   - 101Ã—101 (è®ºæ–‡) vs 100Ã—100 (ä»£ç é»˜è®¤)

4. **æŸå¤±å‡½æ•°å®ç°æ–¹å¼**ï¼š
   - è®ºæ–‡å¼ºè°ƒæŒ‰å…ƒç´ èšåˆï¼Œä»£ç ç›´æ¥å…¨å±€mean
   - æ•°å­¦ä¸Šå¯èƒ½ç­‰ä»·ï¼Œä½†å®ç°ä¸ç¬¦åˆè®ºæ–‡æè¿°

### âš ï¸ éœ€è¦ç¡®è®¤çš„é—®é¢˜ï¼š
1. è®ºæ–‡ä¸­çš„"hidden size"å…·ä½“æŒ‡ä»€ä¹ˆï¼Ÿ
   - å¦‚æœæŒ‡FFNä¸­é—´å±‚ï¼šä»£ç ä¸­`d_ff=2*d_model=256`ï¼Œä»ä¸ç­‰äº512
   - å¦‚æœæŒ‡decoderä¸­çš„hiddenï¼šä»£ç ä¸­`d_hidden=32`ï¼Œè¿œå°äº512
   
2. æŸå¤±å‡½æ•°å®ç°æ˜¯å¦æ•°å­¦ç­‰ä»·ï¼Ÿ
   - å¦‚æœæ‰€æœ‰å…ƒç´ å¤§å°ç›¸åŒä¸”ç‚¹æ•°ç›¸åŒï¼Œå…¨å±€meanç­‰ä»·äºå…ƒç´ å¹³å‡
   - ä½†å¦‚æœå…ƒç´ å¤§å°ä¸åŒï¼Œåº”è¯¥æŒ‰å…ƒç´ é¢ç§¯åŠ æƒ

### ğŸ”§ å»ºè®®ä¿®æ”¹ï¼š
1. **è°ƒæ•´æ¨¡å‹å‚æ•°**ä»¥åŒ¹é…è®ºæ–‡ï¼ˆå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒï¼‰
2. **ä¿®æ”¹é»˜è®¤è®­ç»ƒå‚æ•°**ä»¥åŒ¹é…è®ºæ–‡
3. **ä¿®æ”¹æŸå¤±å‡½æ•°å®ç°**ä»¥æ˜ç¡®ä½“ç°å…ƒç´ çº§åˆ«çš„èšåˆ
4. **ä¿®æ­£æµ‹è¯•ç‚¹æ•°**ä¸º101

