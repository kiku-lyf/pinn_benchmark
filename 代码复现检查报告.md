# SetPINNs 代码复现检查报告

## 一、模型架构参数对比

### 论文 Table \ref{tab:model-hyperparameters}
- **embedding size**: 32
- **hidden size**: 512  
- **\# of encoder**: 1
- **head**: 2
- **set size**: 4

### 代码实现 (`setpinn/models/__init__.py:37`)
```python
SetPinns(d_out=1, d_hidden=32, d_model=128, N=1, heads=2)
```

**❌ 不一致问题：**
- 论文：embedding size = 32, hidden size = 512
- 代码：d_model (embedding) = 128, d_hidden = 32
- **严重不匹配！** 代码的embedding维度是论文的4倍，hidden维度是论文的1/16

**可能的解释：**
- 参数命名不同：论文的"hidden size"可能指FFN的中间层（代码中d_ff = 2*d_model = 256）
- 但embedding size的差异仍然很大（32 vs 128）

---

## 二、训练超参数对比

### 论文 Table \ref{tab:training-hyperparameters}
- **Adam Iterations**: 100
- **L-BFGS Iterations**: 2000
- **Train:Test Split (grid)**: $50\times50$:$101\times101$
- **Train:Test Split (number of samples)**: 2500:10201
- **$\lambda_{\Omega}$**: 1
- **$\lambda_{\Omega_0}$**: 1
- **$\lambda_{\partial\Omega}$**: 1

### 代码默认值 (`train.py`)
- **`--adam_steps`**: 50000 (默认)
- **`--training_iterations`**: 500 (默认，L-BFGS)
- **`--res_points`**: 50 (默认) → 50×50 = 2500 ✓
- **`--test_points`**: 100 (默认) → 100×100 = 10000 ✗ (应该是101)
- **Loss权重**: `sum(losses)` → 等价于 λ=1 ✓

**❌ 不一致问题：**
1. **Adam迭代次数**: 论文100，代码默认50000（差500倍！）
2. **L-BFGS迭代次数**: 论文2000，代码默认500（差4倍）
3. **测试点数**: 论文101×101=10201，代码默认100×100=10000

---

## 三、损失函数实现对比

### 论文 Equation (7) - Localized Residual Energy
$$\mathcal{E}_X(E_k, u_\theta) := \int_{E_k} \bigl\|\mathcal{O}_X(u_\theta)(x)\bigr\|^2 dx \approx \frac{|E_k|}{m_k} \sum_{i=1}^{m_k} \bigl\|\mathcal{O}_X(u_\theta)(x_k^{(i)})\bigr\|^2$$

### 论文 Equation (8) - Total Loss
$$\mathcal{L}_{\text{SetPINNs}} = \sum_{X \in \{\Omega,\, \Omega_0,\, \partial\Omega\}} \frac{\lambda_X}{K} \sum_{k=1}^{K} \mathcal{E}_X(E_k, u_\theta)$$

### 代码实现 (`setpinn/losses.py` 和 `train.py:167`)
```python
# 损失函数中
loss_res = torch.mean((u_tt - 4 * u_xx) ** 2)  # 直接对所有点求mean

# 训练循环中
total_loss = sum(losses)  # 直接相加，相当于λ=1
```

**⚠️ 潜在问题：**
- 论文强调**按元素（set）分别计算损失再平均**
- 代码直接对所有collocation点求mean
- **数学上可能等价**（如果每个set的点数相同且元素大小相同），但**实现方式不符合论文描述**
- 论文明确提到"aggregates residuals within each element"，代码没有体现这一点

**应该的实现方式：**
```python
# 伪代码示意
for each set k:
    set_loss_k = (|E_k|/m_k) * sum(residual^2 for points in set k)
total_loss = mean(set_loss_k for all sets)
```

---

## 四、Element-Aware Sampling (EAS) 实现

### 论文 Definition 2
- 在每个元素$E_k$内均匀采样$m_k$个点
- 要求采样密度均匀：$\frac{m_k}{|E_k|} = \frac{m_j}{|E_j|}$ for all k,j

### 代码实现 (`setpinn/data.py:44-58`)
```python
def eas_sampling(x, t, set_size=4):
    # 遍历网格中的每个正方形
    for i in range(N - 1):
        for j in range(M - 1):
            # 在每个正方形内均匀采样set_size个点
            xs = np.random.uniform(x0, x1, set_size)
            ts = np.random.uniform(t0, t1, set_size)
```

**✓ 基本符合：**
- 实现了在每个元素内均匀采样
- 使用`set_size=4`与论文一致
- 但论文提到"proportional allocation": $m_k = M \frac{|E_k|}{|\Omega|}$，代码中所有元素大小相同（$2\times2$分割），所以自然满足

---

## 五、数据集分割

### 论文
- **Train grid**: $50\times50$ = 2500 points
- **Test grid**: $101\times101$ = 10201 points

### 代码
- **Train**: `res_points=50` → 50×50 = 2500 ✓
- **Test**: `test_points=100` → 100×100 = 10000 ✗

---

## 六、其他设置

### Set Size
- 论文：4 points per element ✓
- 代码：`set_size=4` ✓

### Feed Forward Network
- 论文：未明确说明FFN结构
- 代码：`d_ff = 2 * d_model` (标准Transformer设计) ✓

### Activation Function
- 论文：未明确说明
- 代码：`WaveAct` (sin/cos组合) ✓

### Optimizer Settings
- **L-BFGS**: `line_search_fn="strong_wolfe"` ✓ (与论文一致)
- **Adam**: 使用warmup和cosine scheduler（论文未明确说明，但常见做法）

---

## 七、总结

### ✅ 符合论文的部分：
1. Set size = 4
2. Encoder层数 = 1
3. Attention heads = 2
4. L-BFGS使用strong Wolfe线搜索
5. Loss权重λ=1
6. Element-aware sampling策略正确
7. 训练数据点数 50×50 = 2500

### ❌ 不符合论文的部分：
1. **模型架构参数严重不匹配**：
   - Embedding size: 32 (论文) vs 128 (代码)
   - Hidden size: 512 (论文) vs 32 (代码)
   
2. **训练迭代次数差异巨大**：
   - Adam: 100 (论文) vs 50000 (代码默认)
   - L-BFGS: 2000 (论文) vs 500 (代码默认)
   
3. **测试点数不匹配**：
   - 101×101 (论文) vs 100×100 (代码默认)

4. **损失函数实现方式**：
   - 论文强调按元素聚合，代码直接全局mean
   - 数学上可能等价，但实现不符合论文描述

### ⚠️ 需要确认的问题：
1. 论文中的"hidden size"具体指什么？
   - 如果指FFN中间层：代码中`d_ff=2*d_model=256`，仍不等于512
   - 如果指decoder中的hidden：代码中`d_hidden=32`，远小于512
   
2. 损失函数实现是否数学等价？
   - 如果所有元素大小相同且点数相同，全局mean等价于元素平均
   - 但如果元素大小不同，应该按元素面积加权

### 🔧 建议修改：
1. **调整模型参数**以匹配论文（可能需要重新训练）
2. **修改默认训练参数**以匹配论文
3. **修改损失函数实现**以明确体现元素级别的聚合
4. **修正测试点数**为101

