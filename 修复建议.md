## 问题诊断：训练误差过大

### 当前结果
- Relative L1 error: **1.0646** (损失 > 1表示预测比随机还差)
- Relative L2 error: **1.1199**
- 论文结果: **0.078 ± 0.02** (rRMSE)

### 根本原因分析

**最可能的原因：Adam迭代次数太少**

1. **您使用了100次Adam迭代**（符合论文Table，但可能不够）
2. **训练损失0.4717** 仍然较大，说明模型未充分训练
3. **误差>1** 说明模型基本没有学到有效表示

### 解决方案

**方案1：增加Adam迭代次数（推荐）**
```bash
python train.py \
    --exp_path ./runs \
    --exp_name setpinns-wave-norm \
    --device cuda:0 \
    --res_points 50 \
    --test_points 101 \
    --training_iterations 2000 \
    --use_adam_warmup \
    --adam_steps 10000 \
    --warmup_steps 2000 \
    --adam_lr 1e-3
```

**方案2：尝试更高学习率（100次迭代时）**
```bash
python train.py \
    --exp_path ./runs \
    --exp_name setpinns-wave-norm \
    --device cuda:0 \
    --res_points 50 \
    --test_points 101 \
    --training_iterations 2000 \
    --use_adam_warmup \
    --adam_steps 100 \
    --warmup_steps 0 \
    --adam_lr 5e-3
```

### 检查清单

- [ ] 训练损失是否下降？
  - 如果损失一直在0.4-0.5之间不下降 → 增加迭代次数
  - 如果损失下降但很慢 → 增加学习率
  - 如果损失震荡 → 减小学习率

- [ ] 检查模型参数是否正确
  - ✓ d_model=32, d_hidden=512 (已确认正确)

- [ ] 检查数据形状
  - `x_res` 应该是 [B, S, 1] 格式
  - 边界条件也应该是set格式

### 预期结果

增加Adam迭代次数到10000后，预期：
- 训练损失应该降到 < 0.1
- 相对误差应该降到 < 0.2（接近论文结果）

### 论文中的训练设置

论文Table显示：
- **Adam Iterations**: 100
- **L-BFGS Iterations**: 2000

但这个设置可能：
1. 是针对特定配置优化的
2. 或者论文中实际使用了更多迭代（附录中可能有说明）
3. 或者论文使用了预训练或更好的初始化

**建议：先尝试增加Adam迭代次数，这是最直接有效的方法。**

